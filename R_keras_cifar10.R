#loading keras library
library(keras)
library(imager)
#loading the keras inbuilt cifar10 dataset
cifar<-dataset_cifar10()

library(caret)
trainIndex = createDataPartition(cifar$train$y, 
                                 p=0.8, list=FALSE,times=1)

train_x = cifar$train$x[trainIndex,,,]/255
valid_x = cifar$train$x[-trainIndex,,,]/255

train_y = cifar$train$y[trainIndex,]
valid_y = cifar$train$y[-trainIndex,]

#convert a vector class to binary class matrix
#converting the target variable to once hot encoded vectors using #keras inbuilt function 'to_categorical()
train_y<-to_categorical(train_y,num_classes = 10)
valid_y <- to_categorical(valid_y,num_classes = 10)

#TEST DATA
test_x<-cifar$test$x/255
test_y<-to_categorical(cifar$test$y,num_classes=10) 
#checking the dimentions
dim(train_x) 
cat("No of training samples\t",dim(train_x)[[1]],"\tNo of test samples\t",dim(test_x)[[1]])

#a linear stack of layers
model<-keras_model_sequential()
#configuring the Model
model %>%  
  #defining a 2-D convolution layer
  
  layer_conv_2d(filter=32,kernel_size=c(3,3),padding="same", input_shape=c(32,32,3) ) %>%  
  layer_activation("relu") %>%  
  layer_max_pooling_2d(pool_size=c(2,2)) %>%  
  #another 2-D convolution layer
  
  layer_conv_2d(filter=64 ,kernel_size=c(3,3),padding="same")  %>%  layer_activation("relu") %>%
  #Defining a Pooling layer which reduces the dimentions of the #features map and reduces the computational complexity of the model
  layer_max_pooling_2d(pool_size=c(2,2)) %>%  
  #dropout layer to avoid overfitting
  layer_dropout(0.25) %>%
  layer_conv_2d(filter=128 , kernel_size=c(3,3),padding="same") %>% layer_activation("relu") %>%  
  layer_max_pooling_2d(pool_size=c(2,2)) %>%  
  layer_dropout(0.25) %>%
  
  #flatten the input  
  layer_flatten() %>%  
  layer_dense(512,"relu") %>%  
  # layer_activation() %>%  
  layer_dropout(0.3) %>%  
  
  layer_dense(128,"relu") %>%  
  # layer_activation() %>%  
  layer_dropout(0.5) %>%  
  
  #output layer-10 classes-10 units  
  layer_dense(10) %>%  
  #applying softmax nonlinear activation function to the output layer #to calculate cross-entropy  
  layer_activation("softmax") 
#for computing Probabilities of classes-"logit(log probabilities)

#Model's Optimizer
#defining the type of optimizer-ADAM-Adaptive Momentum Estimation
opt<-optimizer_adam( lr= 0.001 , decay = 1e-6 )
#lr-learning rate , decay - learning rate decay over each update

model %>%
  compile(loss="categorical_crossentropy",
          optimizer=opt,metrics = "accuracy")
#Summary of the Model and its Architecture
summary(model)

#TRAINING PROCESS OF THE MODEL
data_augmentation <- TRUE
if(!data_augmentation) {
  history <- model %>% fit( train_x,train_y ,batch_size=128,
                 epochs=30,validation_data = list(valid_x, valid_y), verbose=2,
                 shuffle=TRUE)
} else {
  #Generating images

  gen_images <- image_data_generator(featurewise_center = FALSE,
                                     featurewise_std_normalization = FALSE,
                                     rotation_range = 20,
                                     width_shift_range = 0.20,
                                     height_shift_range = 0.20,
                                     horizontal_flip = TRUE  )
  #Fit image data generator internal statistics to some sample data
  gen_images %>% fit_image_data_generator(train_x)
  #Generates batches of augmented/normalized data from image data and #labels to visually see the generated images by the Model
  history <- model %>% fit_generator(
    flow_images_from_data(train_x, train_y,gen_images,
                          batch_size=128),#save_to_dir="C:/Users/martin.cheung/Documents/GitHub/Keras CIFAR Testing/"),
    steps_per_epoch=as.integer(40000/128),epochs = 30, verbose=2,
    validation_data = list(valid_x, valid_y) )
}
#use save_to_dir argument to specify the directory to save the #images generated by the Model and to visually check the Model's #output and ability to classify images.


# history <- model %>% fit(
#   train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_data = list(test_x, test_y),
#   shuffle=TRUE
# )

plot(history,type='l')

model %>% evaluate(test_x, test_y)

model %>% predict_classes(test_x)
